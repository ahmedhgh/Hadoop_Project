{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALLATION OF HADOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Jupyter Notebook to save the commands\n",
    "\n",
    "In this segment I will show some basic Hadoop codes. Jupyter notebook can be used to run the teminal (shell) commands by adding to the beginning of the codes the exclamation mark (!),In other words,we can run any commad install any python package using the notebook by simply adding(!) to the beginning of the code (for instance: !pip install pyspark). The nice thing about this environment (Jupyter) we add steps of implementing any project( tracking development),installing a program and save the commands to be used later it is also possible to share the project and collabutrate with others in Github.\n",
    "\n",
    "This is can be done by continuously updating the project (by pushing your project to Github) and make the others track what we are doing. Also it is possible to collaburate in other peoples project by running there code after (pulling it from Github) and giving suggestion to them for any issues. \n",
    "\n",
    "In this notebook, I will show the steps of installing Hadoop and running some basic commands, I will provide another example of creating predictive models using Hadoop and Spark in another notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop: Setting up a Single Node Cluster.\n",
    "The link below is the offical website of Apache Hadoop: \n",
    "https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
    "\n",
    "\n",
    "it provides the information needed to make the installation but skips some of the details we might face in it, I will start with the (Prerequisites). I will demonstrate the UBUNTU 16.04 LTS installation.\n",
    "\n",
    "To install Hadoop we have to have first the following:\n",
    "* Java(TM) SE Runtime Environment (the installed version 1.8.0_171) \n",
    "* Download the latest hadoop version (hadoop-3) -05-Apr-2018 - the size of the file is 311MB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java Installation\n",
    "The following commands needed to paste to the command line(teminal) - we can also run them by adding (!) as mentioned before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step\n",
    "\n",
    "#Installing Java using PPA from the command line (copy and paste to the shell)\n",
    "\n",
    "#1 - Run commands to update system package index and install Java installer script:\n",
    "sudo add-apt-repository ppa:webupd8team/java\n",
    "#2- Update and install the installer script:\n",
    "sudo apt update; sudo apt install oracle-java8-installer\n",
    "\n",
    "#3. Check the Java version\n",
    "java -version\n",
    "\n",
    "#4. Set Java environment variables\n",
    "sudo apt install oracle-java8-set-default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Hadoop (hadoop-3.1.0) from command line\n",
    "sudo wget http://it.apache.contactlab.it/hadoop/common/hadoop-3.1.0/hadoop-3.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Hadoop installation\n",
    "I followed the links listed in the reference to make the installation in my computer, feel free to choose anyone.\n",
    "\n",
    "One thing to notice these are old links I will update there work to the latest version (few path modification)\n",
    "\n",
    "This video is also providing step by step installation(Install Hadoop in ubuntu (Single node cluster)):\n",
    "* https://www.youtube.com/watch?v=Nb1sinaTlmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Adding a dedicated Hadoop user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahmed@laptop:~$ sudo addgroup hadoop\n",
    "Adding group `hadoop' (GID 1002) ...\n",
    "Done.\n",
    "\n",
    "ahmed@laptop:~$ sudo adduser --ingroup hadoop hduser\n",
    "Adding user `hduser' ...\n",
    "Adding new user `hduser' (1001) with group `hadoop' ...\n",
    "Creating home directory `/home/hduser' ...\n",
    "Copying files from `/etc/skel' ...\n",
    "Enter new UNIX password: \n",
    "Retype new UNIX password: \n",
    "passwd: password updated successfully\n",
    "Changing the user information for hduser\n",
    "Enter the new value, or press ENTER for the default\n",
    "\tFull Name []: \n",
    "\tRoom Number []: \n",
    "\tWork Phone []: \n",
    "\tHome Phone []: \n",
    "\tOther []: \n",
    "Is the information correct? [Y/n] Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Installing SSH\n",
    "Add to the shell the following line:\n",
    "\n",
    "\n",
    "$ sudo apt-get install ssh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Create and Setup SSH Certificates\n",
    "Hadoop requires SSH access to manage its nodes, i.e. remote machines plus our local machine. For our single-node setup of Hadoop, we therefore need to configure SSH access to the localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahmed@laptop:~$ su hduser # here I created new user in ubuntu called hduser\n",
    "Password: \n",
    "hduser@laptop:~$ ssh-keygen -t rsa -P \"\" #Adding the Keygen\n",
    "Generating public/private rsa key pair.\n",
    "Enter file in which to save the key (/home/hduser/.ssh/id_rsa): \n",
    "Created directory '/home/hduser/.ssh'.\n",
    "Your identification has been saved in /home/hduser/.ssh/id_rsa.\n",
    "Your public key has been saved in /home/hduser/.ssh/id_rsa.pub.\n",
    "The key fingerprint is:\n",
    "50:6b:f3:fc:0f:32:bf:30:79:c2:41:71:26:cc:7d:e3 hduser@laptop\n",
    "The key's randomart image is:\n",
    "+--[ RSA 2048]----+\n",
    "|        .oo.o    |\n",
    "|       . .o=. o  |\n",
    "|      . + .  o . |\n",
    "|       o =    E  |\n",
    "|        S +      |\n",
    "|         . +     |\n",
    "|          O +    |\n",
    "|           O o   |\n",
    "|            o..  |\n",
    "+-----------------+\n",
    "\n",
    "\n",
    "hduser@laptop:/home/k$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys #Checking the auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Install Hadoop \n",
    "In shell(command line) we add the following codes to install hadoop and put it to the path /usr/local \n",
    "\n",
    "Then we make the path excutable using chown:\n",
    "/usr/local/hadoop-3.1.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ wget http://it.apache.contactlab.it/hadoop/common/hadoop-3.1.0/hadoop-3.1.0.tar.gz\n",
    "$ tar xzf hadoop-3.1.0.tar.gz\n",
    "$ rm -rf hadoop-3.1.0.tar.gz\n",
    "$ mv hadoop-3.1.0.tar.gz /usr/local\n",
    "$ ln -sf /usr/local/hadoop-3.1.0/ /usr/local/hadoop\n",
    "$ chown -R hadoopuser:hadoopgroup /usr/local/hadoop-3.1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to configure some environmental variables,\n",
    "with the hadoopuser account. Switch to that account and edit ~/.bashrc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopuser@ahmed ~ $ editor ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop config\n",
    "#Now add the below code to the end of the editor\n",
    "#APPEND AT THE END: (Do not delete the other available scripts!)\n",
    "\n",
    "export HADOOP_PREFIX=/usr/local/hadoop\n",
    "export HADOOP_HOME=/usr/local/hadoop\n",
    "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
    "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
    "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
    "export YARN_HOME=${HADOOP_HOME}\n",
    "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
    "# Native path\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native\n",
    "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_PREFIX/lib/native\"\n",
    "# Java path\n",
    "export JAVA_HOME=\"/usr\"\n",
    "# OS path\n",
    "export PATH=$PATH:$HADOOP_HOME/bin:$JAVA_PATH/bin:$HADOOP_HOME/sbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Hadoop Configuration\n",
    "The following files will have to be modified to complete the Hadoop setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core-site.xml\n",
    "hdfs-site.xml\n",
    "mapred-site.xml (needs to be copied from mapred-site.xml.template)\n",
    "yarn-site.xml\n",
    "\n",
    "#If it is not permitted to access use (sudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "<property>\n",
    "  <name>fs.default.name</name>\n",
    "    <value>hdfs://localhost:9000</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "<property>\n",
    " <name>dfs.replication</name>\n",
    " <value>1</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "  <name>dfs.name.dir</name>\n",
    "    <value>file:/usr/local/hadoop/hadoopdata/hdfs/namenode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "  <name>dfs.data.dir</name>\n",
    "    <value>file:/usr/local/hadoop/hadoopdata/hdfs/datanode</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapred-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    " <property>\n",
    "  <name>mapreduce.framework.name</name>\n",
    "   <value>yarn</value>\n",
    " </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yarn-site.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    " <property>\n",
    "  <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    " </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Format namenode\n",
    "Next, we need to format the namenode filesystem with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopuser@ahmed ~ $ hdf namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To check the status of the services use the jps command in the shell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopuser@ahmed ~ $ jps\n",
    "26899 Jps\n",
    "26216 SecondaryNameNode\n",
    "25912 NameNode\n",
    "26041 DataNode\n",
    "26378 ResourceManager\n",
    "26494 NodeManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Start and stop Hadoop services\n",
    "Now, the last thing to do is starting Hadoop services:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Hadoop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopuser@ahmed ~ $ start-dfs.sh\n",
    "hadoopuser@ahmed ~ $ start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To stop services, these are the commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Hadoop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopuser@ahmed ~ $ stop-dfs.sh\n",
    "hadoopuser@ahmed ~ $ stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the nodes of the cluster\n",
    "We can check the cluster operation in realtime in our localhost at:http://localhost:8088/cluster/nodes\n",
    "\n",
    "when you press the link, it should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"hadoop.png\">  Nodes of the cluster"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"hadoop.png\">  Nodes of the cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you face any issues you should check the local host port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Hadoop in Jupyter\n",
    "We can also run hadoop commands interactively after the installation using jupyter notebook, and also we can save command and demonstrate projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.\n",
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "httpfs               run HttpFS server, the HDFS HTTP Gateway\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "#Run an HDFS commands\n",
    "!hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run a file system command on the file systems (FsShell):\n",
    "\n",
    "!hdfs dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: HADOOP_PREFIX has been replaced by HADOOP_HOME. Using value of HADOOP_PREFIX.\n",
      "Found 1 items\n",
      "drwxr-xr-x   - hduser supergroup          0 2018-04-30 04:15 /user\n"
     ]
    }
   ],
   "source": [
    "#Showing List of the HDFS root directory:\n",
    "\n",
    "!hdfs dfs -ls /\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refrences:\n",
    "* https://ricma.co/install-apache-hadoop-27-on-buntu-1604.html \n",
    "* https://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/\n",
    "* http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
